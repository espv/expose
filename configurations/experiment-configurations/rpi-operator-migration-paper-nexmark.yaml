# This file describes stream processing experiments.

# Each stream processing system must have a program that:
# - parses this yaml file,
# - interprets the necessary tasks, and
# - inserts tracepoints to trace metrics such as execution time and throughput (optional).

# The only system specific detail that must be placed in this file is SQL queries
# that use a particular syntax for schemas.

name: Experiment configuration

experiments:
  - name: experiment 1 after migration
    id: 24
    flow:
      #- {task: setIntervalBetweenTuples, node: [1], arguments: [40]}
      - {task: setTupleBatchSize, node: [1], arguments: [100]}
      - {task: writeStreamToCsv, arguments: [15, "scripts/Experiments/sink-output/1/15"], node: [3]}
      #- {task: writeStreamToCsv, arguments: [3, "scripts/Experiments/sink-output/1/3"], node: [2]}

      - {task: addSourceNodes, arguments: [11, [1, 2, 3, 4], [1]], node: [2]}
      - {task: addNextHop, arguments: [[1, 2, 3, 4], [4]], node: [1]}

      - {task: addNextHop, arguments: [[15], [3]], node: [2, 4]}

      # Join query
      - {task: deployQueries, arguments: [11, 1], node: [4]}
      #- {task: loadSavepoint, arguments: ["/media/extra-drive1/espenvol/tmp/state/savepoints/received_savepoints/860ba633ddbfe31245fe50f8a2c02d3b/chk-4676"], node: [4]}
      - {task: loadSavepoint, arguments: ["/media/extra-drive1/espenvol/tmp/state/savepoints/received_savepoints/860ba633ddbfe31245fe50f8a2c02d3b/chk-4676"], node: [4]}
      #- {task: loadSavepoint, arguments: ["/media/extra-drive1/espenvol/tmp/state/savepoints/received_savepoints/c822364360af6478db23d0e3b5b6b1b4/chk-152"], node: [4]}
      - {task: startRuntimeEnv, node: [2, 3, 4]}
      - {task: wait, node: [1], arguments: [10000]}
      - { task: sendDsAsStream, arguments: [ 10, 1, false ], node: [1], realism: false}
      # We ensure here that all nodes receive tuples, which means that the migration occurred within a stream
      - {task: retEndOfStream, node: [4], arguments: [1000]}
      - {task: retEndOfStream, node: [3], arguments: [10000]}
      - {task: stopRuntimeEnv, node: [2, 3, 4]}

  - name: experiment 1 --- A1, Small state, with naïve operator migration V1 (one source node)
    id: 11
    flow:
      - {task: setTupleBatchSize, node: [1], arguments: [100]}
      #- {task: writeStreamToCsv, arguments: [3, "scripts/Experiments/sink-output/1/3"], node: [2]}

      - {task: addSourceNodes, arguments: [11, [1, 2, 3, 4], [1]], node: [2]}
      - {task: addNextHop, arguments: [[1, 2, 3, 4], [2]], node: [1]}

      - {task: addNextHop, arguments: [[15], [3]], node: [2, 4]}
      - {task: bufferStream, node: [4], lock-spe: true, arguments: [[1, 2, 3, 4]]}
      - {task: stopStream, node: [4], lock-spe: true, arguments: [[1, 2, 3, 4], 0]}

      # Join query
      - {task: deployQueries, arguments: [11, 1], node: [2]}
      # Warmup: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [0, [
           {task: startRuntimeEnv, node: [2, 3, 4]},
           {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: true},
           {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true},
           {task: retEndOfStream, node: [3], arguments: [2000]},
           {task: stopRuntimeEnv, node: [2, 3, 4]},
         ]]}
      - {task: startRuntimeEnv, node: [2, 3, 4]}
      - {task: traceTuple, node: [2,3], arguments: [0, []]} # Trace that warmup is finished
      # Source node (Node 1) sends dataset number 8 10 times with realism set to false (no wait for ack)
      - {task: sendDsAsStream, arguments: [9, 2000000, false], node: [1], realism: false}
      #- {task: wait, arguments: [10000], node: [2]}
      - {task: retEndOfStream, node: [2], arguments: [20000]}
      #- {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true}
      # Sink node (Node 3) returns from this task when it has received 100,000 tuples
      #- {task: retReceivedXTuples, node: [3], arguments: [1]}
      # Old host (Node 2) starts operator migration
      - {task: batchTasks, node: [2], arguments:
        [[  # Trace start of migration
          #{task: wait, node: [2], arguments: [1000]},
          {task: traceTuple, node: [2], arguments: [3, []]},
          # Old host tells the source node to buffer all the tuples
          #{task: bufferAndStopStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4]]},
          {task: moveStaticQueryState, node: [2], arguments: [11, 4]},
          # This was added by Espen night to 21st of December, during an existing experiment
          {task: sendDsAsStream, arguments: [9, 100000, false], node: [1], realism: false},
          {task: retEndOfStream, node: [2], arguments: [10000]},
          {task: bufferStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4]]},
          {task: stopStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4], 2]},
          {task: relayStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4], [2], [4]]},
          {task: waitForStoppedStreams, node: [2], arguments: [[1], [1, 2, 3, 4]]},
          #{task: wait, node: [2], arguments: [1]},
          # Old host moves the query state to the new host
          {task: moveDynamicQueryState, node: [2], arguments: [11, 4]},
          # Old host tells the source node to resume sending the tuples
          {task: resumeStream, node: [1], arguments: [[1, 2, 3, 4]]},
          # Trace end of migration
          {task: traceTuple, node: [2], arguments: [4, []]},
          {task: wait, node: [2], arguments: [120000]},
          #{task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: true},
          { task: sendDsAsStream, arguments: [ 10, 1, false ], node: [ 1 ], realism: false}
        ]]}
      # We ensure here that all nodes receive tuples, which means that the migration occurred within a stream
      - {task: retEndOfStream, node: [4], arguments: [1000]}
      - {task: retEndOfStream, node: [3], arguments: [20000]}
      - {task: stopRuntimeEnv, node: [2, 3, 4]}
        #      - {task: traceTuple, node: [2,3], arguments: [200, []]}
      # Runs that count: send dataset as stream 5 times
      #- {task: loopTasks,
      #  arguments: [1, [
      #    {task: startRuntimeEnv, node: [2, 3, 4]},
      #    {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false},
      #    {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false},
      #    {task: retEndOfStream, node: [3,4], arguments: [2000]},
      #    {task: stopRuntimeEnv, node: [2, 3, 4]},
      #   ]]
      #}

  - name: experiment 1 --- A1, Aggregate operator, with proper operator migration V1 (one source node)
    id: 19
    flow:
      #- {task: setIntervalBetweenTuples, node: [1], arguments: [40]}
      - {task: setTupleBatchSize, node: [1], arguments: [100]}
      # For join query
      #- {task: writeStreamToCsv, arguments: [15, "scripts/Experiments/sink-output/1/15"], node: [3]}
      - {task: writeStreamToCsv, arguments: [13, "scripts/Experiments/sink-output/1/13"], node: [3]}
      #- {task: writeStreamToCsv, arguments: [3, "scripts/Experiments/sink-output/1/3"], node: [2]}

      - {task: addSourceNodes, arguments: [11, [1, 2, 3, 4], [1]], node: [2]}
      - {task: addNextHop, arguments: [[1, 2, 3, 4], [2]], node: [1]}

      # For the join query
      #- {task: addNextHop, arguments: [[15], [3]], node: [2, 4]}
      - {task: addNextHop, arguments: [[13], [3]], node: [2, 4]}
      - {task: bufferStream, node: [4], lock-spe: true, arguments: [[1, 2, 3, 4]]}
      - {task: stopStream, node: [4], lock-spe: true, arguments: [[1, 2, 3, 4], 0]}

      # Join query
      #- {task: deployQueries, arguments: [11, 1], node: [2]}
      - {task: deployQueries, arguments: [8, 1], node: [2]}
      # Warmup: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [0, [
           {task: startRuntimeEnv, node: [2, 3, 4]},
           {task: sendDsAsStream, arguments: [8, 1, false], node: [1], realism: false, parallel: true},
           {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: true},
           {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true},
           {task: sendDsAsStream, arguments: [11, 1, false], node: [1], realism: false, parallel: true},
           {task: retEndOfStream, node: [3], arguments: [2000]},
           {task: stopRuntimeEnv, node: [2, 3, 4]},
         ]]}
      - {task: startRuntimeEnv, node: [2, 3, 4]}
      - {task: traceTuple, node: [2,3], arguments: [0, []]} # Trace that warmup is finished
      # Source node (Node 1) sends dataset number 8 10 times with realism set to false (no wait for ack)
      - {task: sendDsAsStream, arguments: [11, 1, false], node: [1], realism: false, parallel: true}
      #- {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false}
      #- {task: sendDsAsStream, arguments: [8, 1, false], node: [1], realism: false, parallel: true}
      - {task: retEndOfStream, node: [2], arguments: [2000]}
      #- {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true}
      # Sink node (Node 3) returns from this task when it has received 100,000 tuples
      #- {task: retReceivedXTuples, node: [3], arguments: [1]}
      # Old host (Node 2) starts operator migration
      - {task: batchTasks, node: [2], arguments:
        [[  # Trace start of migration
          #{task: wait, node: [2], arguments: [1000]},
          {task: traceTuple, node: [2], arguments: [3, []]},
          # Old host tells the source node to buffer all the tuples
          #{task: bufferAndStopStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4]]},
          {task: bufferStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4]]},
          {task: stopStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4], 2]},
          {task: relayStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4], [2], [4]]},
          #{task: sendDsAsStream, arguments: [8, 10, false], node: [1], realism: false, parallel: true},
          {task: waitForStoppedStreams, node: [2], arguments: [[1], [1, 2, 3, 4]]},
          #{task: wait, node: [2], arguments: [1]},
          # Old host moves the query state to the new host
          # Join query
          #{task: moveQueryState, node: [2], arguments: [11, 4]},
          {task: moveQueryState, node: [2], arguments: [8, 4]},
          #{ task: sendDsAsStream, arguments: [ 8, 1, false ], node: [ 1 ], realism: false, parallel: true },
          # Old host tells the source node to resume sending the tuples
          {task: resumeStream, node: [1], arguments: [[1, 2, 3, 4]]},
          #{task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: true},
          {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: true},
          #{task: sendDsAsStream, arguments: [11, 1, false], node: [1], realism: false, parallel: true},
          # Trace end of migration
          {task: traceTuple, node: [2], arguments: [4, []]}
        ]]}
      # We ensure here that all nodes receive tuples, which means that the migration occurred within a stream
      - {task: retEndOfStream, node: [3], arguments: [2000]}
      - {task: stopRuntimeEnv, node: [2, 3, 4]}
      - {task: traceTuple, node: [2,3], arguments: [200, []]}
      # Runs that count: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [1, [
           {task: startRuntimeEnv, node: [2, 3, 4]},
           {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true},
           {task: sendDsAsStream, arguments: [11, 1, false], node: [1], realism: false, parallel: true},
           {task: retEndOfStream, node: [3,4], arguments: [2000]},
           {task: stopRuntimeEnv, node: [2, 3, 4]},
         ]]}


  - name: experiment 1 --- A1, Aggregate operator, with proper operator migration V1 (one source node)
    id: 20
    flow:
      #- {task: setIntervalBetweenTuples, node: [1], arguments: [40]}
      - {task: setTupleBatchSize, node: [1], arguments: [100]}
      # For join query
      #- {task: writeStreamToCsv, arguments: [15, "scripts/Experiments/sink-output/1/15"], node: [3]}
      - {task: writeStreamToCsv, arguments: [7, "scripts/Experiments/sink-output/1/13"], node: [3]}
      #- {task: writeStreamToCsv, arguments: [3, "scripts/Experiments/sink-output/1/3"], node: [2]}

      - {task: addSourceNodes, arguments: [4, [1, 2, 3, 4], [1]], node: [2]}
      - {task: addNextHop, arguments: [[1, 2, 3, 4], [2]], node: [1]}

      # For the join query
      #- {task: addNextHop, arguments: [[15], [3]], node: [2, 4]}
      - {task: addNextHop, arguments: [[7], [3]], node: [2, 4]}
      - {task: bufferStream, node: [4], lock-spe: true, arguments: [[1, 2, 3, 4]]}
      - {task: stopStream, node: [4], lock-spe: true, arguments: [[1, 2, 3, 4], 0]}

      # Join query
      #- {task: deployQueries, arguments: [11, 1], node: [2]}
      - {task: deployQueries, arguments: [4, 1], node: [2]}
      # Warmup: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [1, [
           {task: startRuntimeEnv, node: [2, 3, 4]},
           {task: sendDsAsStream, arguments: [8, 1, false], node: [1], realism: false, parallel: true},
           {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: true},
           {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true},
           {task: sendDsAsStream, arguments: [11, 1, false], node: [1], realism: false, parallel: true},
           {task: retEndOfStream, node: [3], arguments: [2000]},
           {task: stopRuntimeEnv, node: [2, 3, 4]},
         ]]}
      - {task: startRuntimeEnv, node: [2, 3, 4]}
      - {task: traceTuple, node: [2,3], arguments: [0, []]} # Trace that warmup is finished
      # Source node (Node 1) sends dataset number 8 10 times with realism set to false (no wait for ack)
      #- {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false}
      - {task: sendDsAsStream, arguments: [8, 1, false], node: [1], realism: false, parallel: true}
      - {task: retEndOfStream, node: [2], arguments: [2000]}
      #- {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true}
      # Sink node (Node 3) returns from this task when it has received 100,000 tuples
      #- {task: retReceivedXTuples, node: [3], arguments: [1]}
      # Old host (Node 2) starts operator migration
      - {task: batchTasks, node: [2], arguments:
        [[  # Trace start of migration
          #{task: wait, node: [2], arguments: [1000]},
          {task: traceTuple, node: [2], arguments: [3, []]},
          # Old host tells the source node to buffer all the tuples
          #{task: bufferAndStopStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4]]},
          {task: batchTasks, node: [1], arguments:
            [[
              { task: bufferStream, node: [ 1 ], lock-spe: true, arguments: [ [ 1, 2, 3, 4 ] ] },
              { task: stopStream, node: [ 1 ], lock-spe: true, arguments: [ [ 1, 2, 3, 4 ], 2 ] },
              { task: relayStream, node: [ 1 ], lock-spe: true, arguments: [ [ 1, 2, 3, 4 ], [ 2 ], [ 4 ] ] },
            ]]},
          #{task: sendDsAsStream, arguments: [8, 10, false], node: [1], realism: false, parallel: true},
          {task: waitForStoppedStreams, node: [2], arguments: [[1], [1, 2, 3, 4]]},
          #{task: wait, node: [2], arguments: [1]},
          # Old host moves the query state to the new host
          # Join query
          #{task: moveQueryState, node: [2], arguments: [11, 4]},
          {task: moveQueryState, node: [2], arguments: [4, 4]},
          # Old host tells the source node to resume sending the tuples
          {task: resumeStream, node: [1], arguments: [[1, 2, 3, 4]]},
          #{task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: true},
          {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true},
          {task: sendDsAsStream, arguments: [11, 1, false], node: [1], realism: false, parallel: true},
          # Trace end of migration
          {task: traceTuple, node: [2], arguments: [4, []]}
        ]]}
      # We ensure here that all nodes receive tuples, which means that the migration occurred within a stream
      - {task: retEndOfStream, node: [3], arguments: [2000]}
      - {task: stopRuntimeEnv, node: [2, 3, 4]}
      - {task: traceTuple, node: [2,3], arguments: [200, []]}
      # Runs that count: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [1, [
           {task: startRuntimeEnv, node: [2, 3, 4]},
           {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: true},
           {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true},
           {task: retEndOfStream, node: [3,4], arguments: [2000]},
           {task: stopRuntimeEnv, node: [2, 3, 4]},
         ]]}

  - name: experiment 2 --- A2, Small state, with good operator migration V1 (one source node)
    id: 12
    flow:
      - {task: setIntervalBetweenTuples, node: [1], arguments: [40]}
        # Old host tells the new host (Node 4) to buffer all tuples from the source node
        # Note that this is not done in Experiment 0
      - {task: stopStream, node: [4], lock-spe: true, arguments: [[1,2,3,4], 0]}
      - {task: bufferStream, node: [4], lock-spe: true, arguments: [[1,2,3,4]]}
      - {task: setTupleBatchSize, node: [1], arguments: [100]}
      - {task: writeStreamToCsv, arguments: [17, "scripts/Experiments/sink-output/1/17"], node: [3]}
      #- {task: writeStreamToCsv, arguments: [3, "scripts/Experiments/sink-output/1/3"], node: [2]}

      - {task: addSourceNodes, arguments: [0, [1,2,3,4], [1]], node: [2]}
      - {task: addNextHop, arguments: [[1,2,3,4], [2]], node: [1]}
      - {task: addNextHop, arguments: [[17], [3]], node: [2,4]}

      # Passthrough query
      - { task: deployQueries, arguments: [ 1, 1 ], node: [ 2 ] }
      # Warmup: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [1, [
         {task: startRuntimeEnv, node: [2,3,4]},
         {task: sendDsAsStream, arguments: [8, 1, false], node: [1], realism: false, parallel: true},
         {task: retEndOfStream, node: [3], arguments: [2000]},
         {task: traceTuple, node: [2], arguments: [200, []]},
         {task: stopRuntimeEnv, node: [2,3,4]},
      ]]}
      - {task: traceTuple, node: [2,3], arguments: [0, []]} # Trace that warmup is finished
      - {task: startRuntimeEnv, node: [2,3,4]}
      # Source node (Node 1) sends dataset number 8 10 times with realism set to false (no wait for ack)
      - {task: sendDsAsStream, arguments: [8, 10, false], node: [1], realism: false, parallel: true}
      # Sink node (Node 3) returns from this task when it has received 100,000 tuples
      #- {task: retReceivedXTuples, node: [3], arguments: [100000]}
      # Old host (Node 2) starts operator migration
      - {task: batchTasks, node: [2], arguments:
         [[  # Trace start of migration
          {task: traceTuple, node: [2], arguments: [3, []]},
          # Old host tells source node to relay the input streams to the new host
          {task: relayStream, node: [1], parallel: true, lock-spe: true, arguments: [[1,2,3,4], [2], [4]]},
          # Old host buffers and stops all streams, both outgoing and incoming streams, related to this query
          {task: bufferStream, node: [2], arguments: [[1,2,3,4,17]]},
          {task: stopStream, node: [2], arguments: [[1,2,3,4,17], 2]},
          # Old host adds new host as a next hop for the input tuples
          {task: addNextHop, node: [2], arguments: [[1,2,3,4], [4]]},
          # Old host moves the query state to the new host
          {task: moveQueryState, node: [2], arguments: [0, 4]},
          # Old host resumes the streams from the input queries
          {task: resumeStream, node: [2], arguments: [[1,2,3,4]]},
          # Trace end of migration
          {task: traceTuple, node: [2], arguments: [4, []]}
      ]]}
      - {task: retEndOfStream, node: [3], arguments: [2000]}
      - {task: stopRuntimeEnv, node: [2,3,4]}
      - {task: traceTuple, node: [2,3,4], arguments: [200, []]}
      # Runs that count: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [1, [
         {task: startRuntimeEnv, node: [2,3,4]},
         {task: sendDsAsStream, arguments: [8, 1, false], node: [1], realism: false, parallel: true},
         {task: retEndOfStream, node: [3], arguments: [2000]},
         {task: traceTuple, node: [2], arguments: [200, []]},
         {task: stopRuntimeEnv, node: [2,3,4]},
      ]]}

  - name: experiment 3 --- A3, Small state, with good operator migration V1 (one source node)
    id: 13
    flow:
      - { task: setIntervalBetweenTuples, node: [ 1 ], arguments: [ 40 ] }
      # Old host tells the new host (Node 4) to buffer all tuples from the source node
      # Note that this is not done in Experiment 0
      - { task: stopStream, node: [ 4 ], lock-spe: true, arguments: [ [ 1,2,3,4 ], 0 ] }
      - { task: bufferStream, node: [ 4 ], lock-spe: true, arguments: [ [ 1,2,3,4 ] ] }
      - { task: setTupleBatchSize, node: [ 1 ], arguments: [ 100 ] }
      - { task: writeStreamToCsv, arguments: [ 17, "scripts/Experiments/sink-output/1/17" ], node: [ 3 ] }
      #- {task: writeStreamToCsv, arguments: [3, "scripts/Experiments/sink-output/1/3"], node: [2]}

      - { task: addSourceNodes, arguments: [ 0, [ 1,2,3,4 ], [ 1 ] ], node: [ 2 ] }
      - { task: addNextHop, arguments: [ [ 1,2,3,4 ], [ 2 ] ], node: [ 1 ] }
      - { task: addNextHop, arguments: [ [ 17 ], [ 3 ] ], node: [ 2,4 ] }

      # Passthrough query
      - { task: deployQueries, arguments: [ 1, 1 ], node: [ 2 ] }
      # Warmup: send dataset as stream 5 times
      - { task: loopTasks,
          arguments: [ 1, [
            { task: startRuntimeEnv, node: [ 2,3,4 ] },
            { task: sendDsAsStream, arguments: [ 8, 1, false ], node: [ 1 ], realism: false, parallel: true },
            { task: retEndOfStream, node: [ 3 ], arguments: [ 2000 ] },
            { task: traceTuple, node: [ 2 ], arguments: [ 200, [ ] ] },
            { task: stopRuntimeEnv, node: [ 2,3,4 ] },
          ] ] }
      - { task: traceTuple, node: [ 2,3 ], arguments: [ 0, [ ] ] } # Trace that warmup is finished
      - { task: startRuntimeEnv, node: [ 2,3,4 ] }
      # Source node (Node 1) sends dataset number 8 10 times with realism set to false (no wait for ack)
      - { task: sendDsAsStream, arguments: [ 8, 10, false ], node: [ 1 ], realism: false, parallel: true }
      # Sink node (Node 3) returns from this task when it has received 100,000 tuples
      #- {task: retReceivedXTuples, node: [3], arguments: [100000]}
      # Old host (Node 2) starts operator migration
      - { task: batchTasks, node: [ 2 ], arguments:
        [ [ # Trace start of migration
          { task: traceTuple, node: [ 2 ], arguments: [ 3, [ ] ] },
          # Old host tells source node to relay the input streams to the new host
          { task: relayStream, node: [ 1 ], parallel: true, lock-spe: true, arguments: [ [ 1,2,3,4 ], [ 2 ], [ 4 ] ] },
          # Old host buffers and stops all streams, both outgoing and incoming streams, related to this query
          { task: bufferStream, node: [ 2 ], arguments: [ [ 1,2,3,4,17 ] ] },
          { task: stopStream, node: [ 2 ], arguments: [ [ 1,2,3,4,17 ], 2 ] },
          # Old host adds new host as a next hop for the input tuples
          { task: addNextHop, node: [ 2 ], arguments: [ [ 1,2,3,4 ], [ 4 ] ] },
          # Old host moves the query state to the new host
          { task: moveQueryState, node: [ 2 ], arguments: [ 0, 4 ] },
          # Old host resumes the streams from the input queries
          { task: resumeStream, node: [ 2 ], arguments: [ [ 1,2,3,4 ] ] },
          # Trace end of migration
          { task: traceTuple, node: [ 2 ], arguments: [ 4, [ ] ] }
        ] ] }
      - { task: retEndOfStream, node: [ 3 ], arguments: [ 2000 ] }
      - { task: stopRuntimeEnv, node: [ 2,3,4 ] }
      - { task: traceTuple, node: [ 2,3,4 ], arguments: [ 200, [ ] ] }
      # Runs that count: send dataset as stream 5 times
      - { task: loopTasks,
          arguments: [ 1, [
            { task: startRuntimeEnv, node: [ 2,3,4 ] },
            { task: sendDsAsStream, arguments: [ 8, 1, false ], node: [ 1 ], realism: false, parallel: true },
            { task: retEndOfStream, node: [ 3 ], arguments: [ 2000 ] },
            { task: traceTuple, node: [ 2 ], arguments: [ 200, [ ] ] },
            { task: stopRuntimeEnv, node: [ 2,3,4 ] },
          ] ] }

  - name: experiment 1 --- A1, Large state, with naïve operator migration V1 (one source node)
    id: 14
    flow:
      - {task: setIntervalBetweenTuples, node: [1], arguments: [40]}
      - {task: setTupleBatchSize, node: [1], arguments: [100]}
      - {task: writeStreamToCsv, arguments: [17, "scripts/Experiments/sink-output/1/17"], node: [3]}
      #- {task: writeStreamToCsv, arguments: [3, "scripts/Experiments/sink-output/1/3"], node: [2]}

      - {task: addSourceNodes, arguments: [11, [1, 2, 3, 4], [1]], node: [2]}
      - {task: addNextHop, arguments: [[1, 2, 3, 4], [2]], node: [1]}

      - {task: addNextHop, arguments: [[15], [3]], node: [2, 4]}
      - {task: bufferStream, node: [4], lock-spe: true, arguments: [[1, 2, 3, 4]]}
      - {task: stopStream, node: [4], lock-spe: true, arguments: [[1, 2, 3, 4], 0]}

      - {task: deployQueries, arguments: [11, 1], node: [2]}
      # Warmup: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [0, [
           {task: startRuntimeEnv, node: [2, 3, 4]},
           {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: false},
           {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: false},
           {task: retEndOfStream, node: [3], arguments: [2000]},
           {task: stopRuntimeEnv, node: [2, 3, 4]},
         ]]}
      - {task: startRuntimeEnv, node: [2, 3, 4]}
      - {task: traceTuple, node: [2,3], arguments: [0, []]} # Trace that warmup is finished
      # Source node (Node 1) sends dataset number 8 10 times with realism set to false (no wait for ack)
      - {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: false}
      - {task: retEndOfStream, node: [2], arguments: [2000]}
      #- {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true}
      # Sink node (Node 3) returns from this task when it has received 100,000 tuples
      #- {task: retReceivedXTuples, node: [3], arguments: [1]}
      # Old host (Node 2) starts operator migration
      - {task: batchTasks, node: [2], arguments:
        [[  # Trace start of migration
          #{task: wait, node: [2], arguments: [1000]},
          {task: traceTuple, node: [2], arguments: [3, []]},
          # Old host tells the source node to buffer all the tuples
          #{task: bufferAndStopStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4]]},
          {task: bufferStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4]]},
          {task: stopStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4], 2]},
          {task: relayStream, node: [1], lock-spe: true, arguments: [[1, 2, 3, 4], [2], [4]]},
          #{task: sendDsAsStream, arguments: [8, 10, false], node: [1], realism: false, parallel: true},
          {task: waitForStoppedStreams, node: [2], arguments: [[1], [1, 2, 3, 4]]},
          #{task: wait, node: [2], arguments: [1]},
          # Old host moves the query state to the new host
          {task: moveQueryState, node: [2], arguments: [4]},
          # Old host tells the source node to resume sending the tuples
          {task: resumeStream, node: [1], arguments: [[1, 2, 3, 4]]},
          #{task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: true},
          {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: false},
          # Trace end of migration
          {task: traceTuple, node: [2], arguments: [4, []]}
        ]]}
      # We ensure here that all nodes receive tuples, which means that the migration occurred within a stream
      - {task: retEndOfStream, node: [3], arguments: [2000]}
      - {task: stopRuntimeEnv, node: [2, 3, 4]}
      - {task: traceTuple, node: [2,3], arguments: [200, []]}
      # Runs that count: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [1, [
           {task: startRuntimeEnv, node: [2, 3, 4]},
           {task: sendDsAsStream, arguments: [9, 1, false], node: [1], realism: false, parallel: true},
           {task: sendDsAsStream, arguments: [10, 1, false], node: [1], realism: false, parallel: true},
           {task: retEndOfStream, node: [3,4], arguments: [2000]},
           {task: stopRuntimeEnv, node: [2, 3, 4]},
         ]]}

  - name: experiment 2 --- A2, Large state, with good operator migration V1 (one source node)
    id: 15
    flow:
      - {task: setIntervalBetweenTuples, node: [1], arguments: [40]}
        # Old host tells the new host (Node 4) to buffer all tuples from the source node
      # Note that this is not done in Experiment 0
      - {task: stopStream, node: [4], lock-spe: true, arguments: [[1,2,3,4], 0]}
      - {task: bufferStream, node: [4], lock-spe: true, arguments: [[1,2,3,4]]}
      - {task: setTupleBatchSize, node: [1], arguments: [100]}
      - {task: writeStreamToCsv, arguments: [17, "scripts/Experiments/sink-output/1/17"], node: [3]}
      #- {task: writeStreamToCsv, arguments: [3, "scripts/Experiments/sink-output/1/3"], node: [2]}

      - {task: addSourceNodes, arguments: [0, [1,2,3,4], [1]], node: [2]}
      - {task: addNextHop, arguments: [[1,2,3,4], [2]], node: [1]}
      - {task: addNextHop, arguments: [[17], [3]], node: [2,4]}

      - {task: deployQueries, arguments: [0, 1], node: [2]}
      # Warmup: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [1, [
           {task: startRuntimeEnv, node: [2,3,4]},
           {task: sendDsAsStream, arguments: [8, 1, false], node: [1], realism: false, parallel: true},
           {task: retEndOfStream, node: [3], arguments: [2000]},
           {task: traceTuple, node: [2], arguments: [200, []]},
           {task: stopRuntimeEnv, node: [2,3,4]},
         ]]}
      - {task: traceTuple, node: [2,3], arguments: [0, []]} # Trace that warmup is finished
      - {task: startRuntimeEnv, node: [2,3,4]}
      # Source node (Node 1) sends dataset number 8 10 times with realism set to false (no wait for ack)
      - {task: sendDsAsStream, arguments: [8, 10, false], node: [1], realism: false, parallel: true}
      # Sink node (Node 3) returns from this task when it has received 100,000 tuples
      #- {task: retReceivedXTuples, node: [3], arguments: [100000]}
      # Old host (Node 2) starts operator migration
      - {task: batchTasks, node: [2], arguments:
        [[  # Trace start of migration
          {task: traceTuple, node: [2], arguments: [3, []]},
          # Old host tells source node to relay the input streams to the new host
          {task: relayStream, node: [1], parallel: true, lock-spe: true, arguments: [[1,2,3,4], [2], [4]]},
          # Old host buffers and stops all streams, both outgoing and incoming streams, related to this query
          {task: bufferStream, node: [2], arguments: [[1,2,3,4,17]]},
          {task: stopStream, node: [2], arguments: [[1,2,3,4,17], 2]},
          # Old host adds new host as a next hop for the input tuples
          {task: addNextHop, node: [2], arguments: [[1,2,3,4], [4]]},
          # Old host moves the query state to the new host
          {task: moveQueryState, node: [2], arguments: [0, 4]},
          # Old host resumes the streams from the input queries
          {task: resumeStream, node: [2], arguments: [[1,2,3,4]]},
          # Trace end of migration
          {task: traceTuple, node: [2], arguments: [4, []]}
        ]]}
      - {task: retEndOfStream, node: [3], arguments: [2000]}
      - {task: stopRuntimeEnv, node: [2,3,4]}
      - {task: traceTuple, node: [2,3,4], arguments: [200, []]}
      # Runs that count: send dataset as stream 5 times
      - {task: loopTasks,
         arguments: [1, [
           {task: startRuntimeEnv, node: [2,3,4]},
           {task: sendDsAsStream, arguments: [8, 1, false], node: [1], realism: false, parallel: true},
           {task: retEndOfStream, node: [3], arguments: [2000]},
           {task: traceTuple, node: [2], arguments: [200, []]},
           {task: stopRuntimeEnv, node: [2,3,4]},
         ]]}

      - name: experiment 3 --- A3, Large state, with good operator migration V1 (one source node)
        id: 16
        flow:
          - { task: setIntervalBetweenTuples, node: [ 1 ], arguments: [ 40 ] }
          # Old host tells the new host (Node 4) to buffer all tuples from the source node
          # Note that this is not done in Experiment 0
          - { task: stopStream, node: [ 4 ], lock-spe: true, arguments: [ [ 1,2,3,4 ], 0 ] }
          - { task: bufferStream, node: [ 4 ], lock-spe: true, arguments: [ [ 1,2,3,4 ] ] }
          - { task: setTupleBatchSize, node: [ 1 ], arguments: [ 100 ] }
          - { task: writeStreamToCsv, arguments: [ 17, "scripts/Experiments/sink-output/1/17" ], node: [ 3 ] }
          #- {task: writeStreamToCsv, arguments: [3, "scripts/Experiments/sink-output/1/3"], node: [2]}

          - { task: addSourceNodes, arguments: [ 0, [ 1,2,3,4 ], [ 1 ] ], node: [ 2 ] }
          - { task: addNextHop, arguments: [ [ 1,2,3,4 ], [ 2 ] ], node: [ 1 ] }
          - { task: addNextHop, arguments: [ [ 17 ], [ 3 ] ], node: [ 2,4 ] }

          - { task: deployQueries, arguments: [ 0, 1 ], node: [ 2 ] }
          # Warmup: send dataset as stream 5 times
          - { task: loopTasks,
              arguments: [ 1, [
                { task: startRuntimeEnv, node: [ 2,3,4 ] },
                { task: sendDsAsStream, arguments: [ 8, 1, false ], node: [ 1 ], realism: false, parallel: true },
                { task: retEndOfStream, node: [ 3 ], arguments: [ 2000 ] },
                { task: traceTuple, node: [ 2 ], arguments: [ 200, [ ] ] },
                { task: stopRuntimeEnv, node: [ 2,3,4 ] },
              ] ] }
          - { task: traceTuple, node: [ 2,3 ], arguments: [ 0, [ ] ] } # Trace that warmup is finished
          - { task: startRuntimeEnv, node: [ 2,3,4 ] }
          # Source node (Node 1) sends dataset number 8 10 times with realism set to false (no wait for ack)
          - { task: sendDsAsStream, arguments: [ 8, 10, false ], node: [ 1 ], realism: false, parallel: true }
          # Sink node (Node 3) returns from this task when it has received 100,000 tuples
          #- {task: retReceivedXTuples, node: [3], arguments: [100000]}
          # Old host (Node 2) starts operator migration
          - { task: batchTasks, node: [ 2 ], arguments:
            [ [ # Trace start of migration
              { task: traceTuple, node: [ 2 ], arguments: [ 3, [ ] ] },
              # Old host tells source node to relay the input streams to the new host
              { task: relayStream, node: [ 1 ], parallel: true, lock-spe: true, arguments: [ [ 1,2,3,4 ], [ 2 ], [ 4 ] ] },
              # Old host buffers and stops all streams, both outgoing and incoming streams, related to this query
              { task: bufferStream, node: [ 2 ], arguments: [ [ 1,2,3,4,17 ] ] },
              { task: stopStream, node: [ 2 ], arguments: [ [ 1,2,3,4,17 ], 2 ] },
              # Old host adds new host as a next hop for the input tuples
              { task: addNextHop, node: [ 2 ], arguments: [ [ 1,2,3,4 ], [ 4 ] ] },
              # Old host moves the query state to the new host
              { task: moveQueryState, node: [ 2 ], arguments: [ 0, 4 ] },
              # Old host resumes the streams from the input queries
              { task: resumeStream, node: [ 2 ], arguments: [ [ 1,2,3,4 ] ] },
              # Trace end of migration
              { task: traceTuple, node: [ 2 ], arguments: [ 4, [ ] ] }
            ] ] }
          - { task: retEndOfStream, node: [ 3 ], arguments: [ 2000 ] }
          - { task: stopRuntimeEnv, node: [ 2,3,4 ] }
          - { task: traceTuple, node: [ 2,3,4 ], arguments: [ 200, [ ] ] }
          # Runs that count: send dataset as stream 5 times
          - { task: loopTasks,
              arguments: [ 1, [
                { task: startRuntimeEnv, node: [ 2,3,4 ] },
                { task: sendDsAsStream, arguments: [ 8, 1, false ], node: [ 1 ], realism: false, parallel: true },
                { task: retEndOfStream, node: [ 3 ], arguments: [ 2000 ] },
                { task: traceTuple, node: [ 2 ], arguments: [ 200, [ ] ] },
                { task: stopRuntimeEnv, node: [ 2,3,4 ] },
              ] ] }

spequeries:
  - name: "Passthrough (NexMark query 0)"
    id: 0
    output-stream-id: 17  # Bid
    type: fetch-query
    sql-query:
      t-rex: "Assign 3 => Bid, 17 => OutBid
              Define OutBid(auction: int, bidder: int, price: int, dateTime: int, dateTime2: int)
              Bid() as B
              Where auction := B.auction, bidder := B.seller, price := B.price, dateTime := B.dateTime, dateTime2 := B.dateTime2
              Consuming B;"
      siddhi: "from Bid
               select *
               insert into OutBid;"
      beam: "select *
             from Bid"
      flink: "select *
              from Bid"
      esper: "insert into OutBid
              select *
              from Bid;"
      template: ""

  - name: "Passthrough (NexMark query 0)"
    id: 1
    output-stream-id: 17  # OutBid
    type: fetch-query
    sql-query:
      siddhi: "from Bid
               select auction, bidder, udf:doltoeur(price) as price, dateTime, dateTime2
               insert into OutBid;"
      beam: "select auction, bidder, DOLTOEUR(price) as price, eventTime, dateTime2
             from Bid"
      flink: "select auction, bidder, DOLTOEUR(price) as price, eventTime, dateTime2
              from Bid"
      esper: "insert into OutBid
              select doltoeur(price) as price
              from Bid;"
      template: ""

  - name: "Selection (NexMark query 2)"
    id: 2
    output-stream-id: 5
    type: fetch-query
    sql-query:
      # T-Rex lacks the 'or' operator for attribute constraints
      t-rex: ""
      siddhi: "from Bid[auction == 1007 or auction == 1020 or auction == 2001 or auction == 2019 or auction == 2087]
               select auction, price
               insert into OutQuery2;"
      beam: "select auction, price
             from Bid
             where auction = 1007 or auction = 1020 or auction = 2001 or auction = 2019 or auction = 2087"
      flink: "select auction, price
              from Bid
              where auction = 1007 or auction = 1020 or auction = 2001 or auction = 2019 or auction = 2087"
      esper: "insert into OutQuery2
              select auction, price
              from Bid
              where auction = 1007 or auction = 1020 or auction = 2001 or auction = 2019 or auction = 2087;"
      template: "SELECT Rstream(auction, price)
                 FROM Bid [NOW]
                 WHERE auction = 1007 OR auction = 1020 OR auction = 2001 OR auction = 2019 OR auction = 2087;"

  - name: "Local Item Suggestion (NexMark query 3)"
    id: 3
    output-stream-id: 6
    type: fetch-query
    sql-query:
      t-rex: ""
      siddhi: "from Auction[category == 10]#window.time(99999 years) as A
               join Person[state == 'OR' or state == 'ID' or state == 'CA']#window.time(99999 years) as P on A.seller == P.id
               select P.name, P.city, P.state, A.id
               insert into OutQuery3;"
      beam: "select P.name, P.city, P.state, A.id
             from Auction A, Person P
             where A.seller = P.id and (P.state = 'OR' or P.state = 'ID' or P.state = 'CA') and A.category = 10"
      flink: "select P.name, P.city, P.state, A.id
              from Auction A, Person P
              where A.seller = P.id and (P.state = 'OR' or P.state = 'ID' or P.state = 'CA') and A.category = 10"
      esper: "insert into OutQuery3
              select P.name as name, P.city as city, P.state as state, A.id as id
              from Auction#time(999 minutes) A, Person#time(999 minutes) P
              where A.seller = P.id and (P.state = 'OR' or P.state = 'ID' or P.state = 'CA') and A.category = 10;"
      template: "SELECT Istream(P.name, P.city, P.state, A.id)
                 FROM Auction A [ROWS UNBOUNDED], Person P [ROWS UNBOUNDED]
                 WHERE A.seller = P.id AND (P.state = `OR' OR P.state = `ID' OR P.state = `CA') AND A.category = 10;"

  - name: "Average Price for a Category (NexMark query 4)"
    id: 4
    output-stream-id: 7
    type: fetch-query
    sql-query:
      t-rex: ""
      siddhi: "from Bid#window.time(999 years) as B
               join Auction#window.time(999 years) as A on A.id == B.auction
               select B.dateTime, B.price, A.category, B.auction, A.expires
               insert into MidQuery4_1;

               from MidQuery4_1#window.externalTimeBatch(dateTime, 1 minute)[dateTime < expires]
               select max(price) as final, category
               group by auction, category
               insert into MidQuery4_2;

               from MidQuery4_2#window.time(999 years)
               select convert(avg(final), 'long') as price, category
               group by category
               insert into OutQuery4;"
      beam: "select avg(final), category
             from (SELECT MAX(B.price) AS final, A.category
                   FROM Auction A, Bid B
                   WHERE A.id=B.auction AND B.dateTime2 < A.expires2
                   GROUP BY A.id, A.category) Q
             group by category"
      flink: "select avg(final), category
              from (SELECT MAX(B.price) AS final, A.category
                    FROM Auction A, Bid B
                    WHERE A.id=B.auction AND B.dateTime2 < A.expires2
                    GROUP BY A.id, A.category) Q
              group by category"
      esper: "insert into MidQuery4_2
              select max(B.price) as final, A.category as category
              from Auction#time(999 minutes) A, Bid#ext_timed_batch(dateTime, 1 minute) B
              where A.id = B.auction and B.dateTime < A.expires
              group by B.auction, A.category;

              insert into OutQuery4
              select cast(avg(final), long) as price, category
              from MidQuery4_2
              group by category;"
      template: "SELECT Istream(AVG(Q.final))
                 FROM Category C, (SELECT Rstream(MAX(B.price) AS final, A.category)
                                   FROM Auction A [ROWS UNBOUNDED], Bid B [ROWS UNBOUNDED]
                                   WHERE A.id=B.auction AND B.dateTime < A.expires AND A.expires < CURRENT_TIME
                                   GROUP BY A.id, A.category) Q
                 WHERE Q.category = C.id
                 GROUP BY C.id;"

  - name: "MidQuery5_1"
    id: 5
    output-stream-id: 9
    type: fetch-query
    sql-query:
      t-rex: ""
      siddhi: ""
      flink: "select auction, count(*) as num
              from Bid
              group by auction"
      esper: ""

  - name: "Hot Items (NexMark query 5) (Might not be possible)"
    id: 7
    output-stream-id: 8
    type: fetch-query
    sql-query:
      t-rex: ""
      siddhi: "from Bid#window.externalTimeBatch(dateTime, 1 minute)
               select auction, count(*) as num
               group by auction
               insert into MidQuery5_1;

               from MidQuery5_1
               select auction, num as maxnum
               having maxnum == max(maxnum)
               insert into OutQuery5;"
      beam: "SELECT Q1.auction, Q2.maxnum
             FROM (SELECT B1.auction, count(*) AS num
                   FROM Bid B1
                   GROUP BY B1.auction) Q1,
                  (select max(num) as maxnum
                   from (SELECT B1.auction, count(*) AS num
                         FROM Bid B1
                         GROUP BY B1.auction)) Q2
             WHERE Q1.num = Q2.maxnum"
      flink: "SELECT Q1.auction, Q2.maxnum
              FROM (SELECT B1.auction, count(*) AS num
                    FROM Bid B1
                    GROUP BY B1.auction) Q1,
                   (select max(num) as maxnum
                    from (SELECT B1.auction, count(*) AS num
                          FROM Bid B1
                          GROUP BY B1.auction)) Q2
              WHERE Q1.num = Q2.maxnum"
      #"select M1.auction, M2.maxnum
      # from MidQuery5_1 M1 join MidQuery5_2 M2 on M1.num = M2.maxnum"
      esper: "insert into MidQuery5_1
              select auction, count(*) as num
              from Bid#ext_timed_batch(dateTime, 1 minute)
              group by auction;

              insert into OutQuery5
              select auction, num as maxnum
              from MidQuery5_1
              where num >= all (select count(*)
                                from Bid#ext_timed_batch(dateTime, 1 minute) B2
                                group by B2.auction);"

      template: "SELECT Rstream(auction)
                 FROM (SELECT B1.auction, count(*) AS num
                       FROM Bid [RANGE 60 MINUTE SLIDE 1 MINUTE] B1
                       GROUP BY B1.auction)
                 WHERE num >= ALL (SELECT count(*)
                                   FROM Bid [RANGE 60 MINUTE SLIDE 1 MINUTE] B2
                                   GROUP BY B2.auction);"

  - name: "Average Selling Price by Seller (NexMark query 6)"
    id: 8
    output-stream-id: 13
    sql-query:
      t-rex: "
Assign 2 => Auction, 3 => Bid, 11 => MidQuery6
Define MidQuery6(auction: int, seller: int, final: int, dateTime: int)
From Auction() as A and
each Bid([int] auction=A.id, [int] dateTime < A.expires) as B within 500000000 from A
Where auction := A.id, seller := A.seller, final := B.price, dateTime := B.dateTime
Consuming A, B;

Assign 2 => Auction, 3 => Bid, 11 => MidQuery6
Define MidQuery6(auction: int, seller: int, final: int, dateTime: int)
From Bid() as B and
each Auction([int] id=B.auction, [int] expires > B.dateTime) as A within 500000000 from A
Where auction := A.id, seller := A.seller, final := B.price, dateTime := B.dateTime
Consuming A, B;

Assign 11 => MidQuery6, 13 => OutQuery6
Define OutQuery6(auction: int, seller: int, final: int, dateTime: int)
From MidQuery6() as M
Where auction := M.auction, seller := M.seller, final := M.price, dateTime := M.dateTime
Consuming M;
"  # Bid and MidQuery6 get consumed because they will only be matched once
      siddhi: "from Bid#window.time(999 years) as B
                 join Auction#window.time(999 years) as A
                 on A.id == B.auction
               select price, seller, auction, dateTime, expires
               insert into MidQuery6_1;

               from MidQuery6_1#window.externalTimeBatch(dateTime, 1 minute)[dateTime < expires]
               select max(price) as final, seller
               group by auction, seller
               insert into MidQuery6_2;

               from MidQuery6_2
               select convert(avg(final), 'long') as final, seller
               group by seller
               insert into OutQuery6;"
      flink: "SELECT AVG(Q.final), Q.seller
              FROM (SELECT MAX(B.price) AS final, A.seller
                    FROM Auction A, Bid B
                    WHERE A.id=B.auction AND B.dateTime2 < A.expires2
                    GROUP BY A.id, A.seller) Q
              GROUP BY Q.seller"
      beam: "SELECT AVG(Q.final), Q.seller
             FROM (SELECT MAX(B.price) AS final, A.seller
                   FROM Auction A, Bid B
                   WHERE A.id=B.auction AND B.dateTime2 < A.expires2
                   GROUP BY A.id, A.seller) Q
             GROUP BY Q.seller"
      esper: "insert into MidQuery6_1
              select price, seller, auction, dateTime, expires
              from Auction#win:time(999 years) as A, Bid#win:time(999 years) as B
              where A.id = B.auction
              group by A.id, A.seller;

              insert into MidQuery6_2
              select max(price) as final, seller
              from MidQuery6_1#win:ext_timed_batch(dateTime, 1 minute)
              where dateTime < expires
              group by auction, seller;

              insert into OutQuery6
              select cast(avg(final), long) as final, seller
              from MidQuery6_2#win:time(999 years)
              group by seller;"
      template_sql: "SELECT Istream(AVG(Q.final), Q.seller)
                     FROM (SELECT Rstream(MAX(B.price) AS final, A.seller)
                           FROM Auction A [ROWS UNBOUNDED], Bid B [ROWS UNBOUNDED]
                           WHERE A.id=B.auction AND B.dateTime < A.expires AND A.expires < CURRENT_TIME
                           GROUP BY A.id, A.seller) [PARTITION BY A.seller ROWS 10] Q
                     GROUP BY Q.seller;"

  - name: "MidQuery7"
    id: 9
    output-stream-id: 16
    sql-query:
      beam: "select max(price) as price
             from Bid"

  - name: "Highest Bid (NexMark query 7) (Might not be possible)"
    id: 10
    output-stream-id: 14
    sql-query:
      t-rex: ""
      siddhi: "from Bid#window.externalTimeBatch(dateTime, 1 minute)
               select max(price) as price
               insert into MidQuery7;

               from MidQuery7#window.time(999 years) as M
               join Bid#window.time(999 years) as B on B.price == M.price
               select B.auction, M.price, B.bidder
               insert into OutQuery7;"
      beam: "SELECT B.auction, B.price, B.bidder
             FROM Bid B, MidQuery7 M
             WHERE M.price = B.price"
      flink: "SELECT B.auction, B.price, B.bidder
              FROM Bid B
              WHERE B.price = (SELECT MAX(B1.price)
                               FROM Bid B1)"
      esper: "insert into OutQuery7
              select auction, price, bidder
              from Bid#ext_timed_batch(dateTime, 1 minute) as B
              where price = (select max(B1.price)
                             from Bid#ext_timed_batch(dateTime, 1 minute) as B1);"
      template: "SELECT Rstream(B.auction, B.price, B.bidder)
                 FROM Bid [RANGE 1 MINUTE SLIDE 1 MINUTE] B
                 WHERE B.price = (SELECT MAX(B1.price)
                                  FROM BID [RANGE 1 MINUTE SLIDE 1 MINUTE] B1);"

  - name: "Monitor New Users (NexMark query 8) (Might not be possible)"
    id: 11
    comment: "We modify the template query by removing the window. That's
              because the dataset only spans 10 seconds, and so a 12 hour window
              will include all tuples anyway. Since T-Rex doesn't support external
              time windows, T-Rex can then run this query."
    output-stream-id: 15
    sql-query:
      t-rex: "
Assign 1 => Person, 2 => Auction, 15 => OutQuery8
Define  OutQuery8(id: int, name: string, reserve: int)
From   Person() as P and
each   Auction([int] seller=P.id) as A within 5000 from P
Where id := P.id, name := P.name, reserve := reserve
Consuming A; "  # Auction is consumed because they will only be matched once
      siddhi: "from Auction#window.time(999 years) as A
               join Person#window.time(999 years) as P on P.id == A.seller
               select P.id, P.name, A.reserve
               insert into OutQuery8;"
      flink: "select P.id, A.itemName, A.reserve
              from Auction A
              join Person P on P.id = A.seller"
      flink2: "select A.initialBid, A.itemName, A.description, A.initialBid, A.reserve, A.expires2,
                      A.seller, A.category, A.description, A.description, A.description, A.description
                    from Auction A
                    join Person P on P.id = A.seller"
      flink3: "select SUM(A.initialBid), A.itemName, A.itemName, A.initialBid, A.reserve, A.expires2,
                                 A.seller, A.category, A.itemName, A.itemName, A.itemName, A.itemName
                          from Auction A
                          group by A.itemName, A.initialBid, A.reserve, A.expires2,
                                 A.seller, A.category"
      beam: "select P.id, P.name, A.reserve
             from Auction A
             join Person P on P.id = A.seller"
      esper: "insert into OutQuery8
              select P.id as id, P.name as name, A.reserve as reserve
              from Auction#time(999 years) as A, Person#time(999 years) as P
              where P.id = A.seller;"
      template: "SELECT Rstream(P.id, P.name, A.reserve)
                 FROM Person [RANGE 12 HOUR] P, Auction [RANGE 12 HOUR] A
                 WHERE P.id = A.seller;"

stream-definitions:
  - id: 0
    stream-id: 0
    name: StopStream
    tuple-format: [{name: streamList, type: string}, {name: node, type: int}]

  - id: 1
    stream-id: 1
    name: Person
    tuple-format: [{name: id, type: int}, {name: name, type: string}, {name: emailAddress, type: string},
                   {name: creditCard, type: string}, {name: city, type: string}, {name: state, type: string}]

  - id: 2
    stream-id: 2
    name: Auction
    rowtime-column: {column: expires, nanoseconds-per-tick: 1000000000}
    tuple-format: [{name: id, type: int}, {name: itemName, type: string}, {name: description, type: string},
                   {name: initialBid, type: long}, {name: reserve, type: int}, {name: expires, type: long-timestamp},
                   {name: expires2, type: long}, {name: seller, type: int}, {name: category, type: int}]

  - id: 3
    stream-id: 3
    name: Bid
    rowtime-column: {column: dateTime, nanoseconds-per-tick: 1000000000}
    tuple-format: [{name: auction, type: int}, {name: bidder, type: int}, {name: price, type: long},
                   {name: dateTime, type: long-timestamp}, {name: dateTime2, type: long}]

  - id: 4
    stream-id: 4
    name: Category
    tuple-format: [{name: id, type: int}, {name: name, type: string}, {name: description, type: string},
                   {name: parentCategory, type: int}]

  - id: 5
    stream-id: 5
    name: OutQuery2
    tuple-format: [{name: auction, type: int}, {name: price, type: long}]

  - id: 6
    stream-id: 6
    name: OutQuery3
    tuple-format: [{name: name, type: string}, {name: city, type: string}, {name: state, type: string},
                   {name: id, type: int}]

  - id: 7
    stream-id: 7
    name: OutQuery4
    tuple-format: [{name: price, type: long}, {name: category, type: int}]

  - id: 53
    stream-id: 53
    name: MidQuery4_1
    tuple-format: [{name: dateTime, type: long-timestamp}, {name: price, type: long}, {name: category, type: int}, {name: auction, type: int},
                   {name: expires, type: long-timestamp}]

  - id: 63
    stream-id: 63
    name: MidQuery4_2
    tuple-format: [{name: final, type: long}, {name: category, type: int}]

  - id: 8
    stream-id: 8
    name: OutQuery5
    tuple-format: [{name: auction, type: int}, {name: maxnum, type: long}]

  - id: 9
    stream-id: 9
    name: MidQuery5_1
    intermediary-stream: true
    tuple-format: [{name: auction, type: int}, {name: num, type: long}]

  - id: 10
    stream-id: 10
    name: MidQuery5_2
    intermediary-stream: true
    tuple-format: [{name: maxnum, type: long}]

  - id: 11
    stream-id: 11
    name: MidQuery6_1
    intermediary-stream: true
    tuple-format: [{name: price, type: long}, {name: seller, type: int}, {name: auction, type: int}, {name: dateTime, type: long-timestamp},
                   {name: expires, type: long-timestamp}]

  - id: 12
    stream-id: 12
    name: MidQuery6_2
    intermediary-stream: true
    tuple-format: [{name: final, type: long}, {name: seller, type: int}]

  - id: 13
    stream-id: 13
    name: OutQuery6
    tuple-format: [{name: final, type: long}, {name: seller, type: int}]

  - id: 14
    stream-id: 14
    name: OutQuery7
    tuple-format: [{name: auction, type: int}, {name: price, type: long}, {name: bidder, type: int}]

  - id: 15
    stream-id: 15
    name: OutQuery8
    tuple-format: [{name: id, type: int}, {name: name, type: string}, {name: reserve, type: int}]

#  - id: 15
#    stream-id: 15
#    name: OutQuery8
#    tuple-format: [ { name: id, type: long }, { name: name, type: string },
#                    { name: itemName, type: string }, { name: initialBid, type: long }, { name: reserve, type: int },
#                    { name: expires2, type: long }, { name: seller, type: int }, { name: category, type: int },
#                    { name: emailAddress, type: string }, { name: creditCard, type: string }, { name: city, type: string },
#                    { name: state, type: string } ]

  - id: 16
    stream-id: 16
    name: MidQuery7
    intermediary-stream: true
    tuple-format: [{name: price, type: long}]

  - id: 17
    stream-id: 17
    name: OutBid
    rowtime-column: {column: dateTime, nanoseconds-per-tick: 1000000000}
    tuple-format: [{name: auction, type: int}, {name: bidder, type: int}, {name: price, type: long},
                   {name: dateTime, type: long-timestamp}, {name: dateTime2, type: long}]


datasets:
  - name: NexMark 40,000 tuples (YAML)
    type: yaml
    id: 8
    file: Datasets/NexMark/nexmark-dataset-100k.yaml
  - name: NexMark 40,000 tuples (YAML)
    type: yaml
    id: 9
    file: Datasets/NexMark/nexmark-dataset-auction.yaml
  - name: NexMark 40,000 tuples (YAML)
    type: yaml
    id: 10
    file: Datasets/NexMark/nexmark-dataset-person.yaml
  - name: NexMark 40,000 tuples (YAML)
    type: yaml
    id: 11
    file: Datasets/NexMark/nexmark-dataset-bid-large.yaml


tracepoints:
  - id: 0
    name: Start experiment
    active: true
    category:
      isScalingEvent: false
      isMilestoneEvent: false

  - id: 1
    name: Receive Event
    active: true
    arguments:
      - name: tid
        type: int
      - name: CurCepEvent
        type: int
    description:
    category:
      isScalingEvent: false
      isMilestoneEvent: true

  - id: 2
    name: Transmit event
    active: true
    arguments:
      - name: tid
        type: int
      - name: CurCepEvent
        type: int
    description:
    category:
      isScalingEvent: false
      isMilestoneEvent: true

  - id: 3
    name: Start operator migration
    active: true
    arguments:
      - name: tid
        type: int
    description:
    category:
      isScalingEvent: false
      isMilestoneEvent: true

  - id: 4
    name: Stop operator migration
    active: true
    arguments:
      - name: tid
        type: int
    description:
    category:
      isScalingEvent: false
      isMilestoneEvent: true

  - id: 6
    name: Created Complex Event
    active: false
    arguments:
      - name: tid
        type: int
      - name: CurCepEvent
        type: int
    description: A complex event was created
    category:
      isScalingEvent: false
      isMilestoneEvent: true

  - id: 100
    name: Finished Processing Event
    active: true
    arguments:
      - name: tid
        type: int
      - name: CurCepEvent
        type: int
    description:
    category:
      isScalingEvent: false
      isMilestoneEvent: true

  - id: 221
    name: Add Query
    active: true
    arguments:
      - name: CepQuery
        type: int
    x_variable: numberQueries
    description: Traced when deploying a query. This is a simulation and scaling event.
    category:
      isScalingEvent: true
      isMilestoneEvent: false

  - id: 200
    name: Finished one set
    active: true
    description: Traced when one set of streams have ended
    category:
      isScalingEvent: false
      isMilestoneEvent: false

  - id: 201
    name: Increase number of subscribers
    active: true
    description: Traced when the number of subscribers increases
    category:
      isScalingEvent: true
      isMilestoneEvent: false

  - id: 202
    name: Increase number of publishers
    active: true
    description: Traced when the number of publishers increases
    category:
      isScalingEvent: true
      isMilestoneEvent: false

  - id: 203
    name: Increase number of publishers and subscribers
    active: true
    description: Traced when the number of publishers and subscribers increases
    category:
      isScalingEvent: true
      isMilestoneEvent: false
